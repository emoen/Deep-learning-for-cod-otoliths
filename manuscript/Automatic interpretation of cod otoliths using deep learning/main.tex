\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}

\usepackage{float}

% Remove comment for double spacing
\usepackage{setspace} 
\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{wrapfig}
% \usepackage[nomarkers,figuresonly]{endfloat}
\usepackage[caption=false]{subfig}

% for editing
\usepackage[normalem]{ulem}

\newcommand\Mycite[1]{%
  \citeauthor{#1}~[\citeyear{#1}]}



\begin{document}
\vspace*{0.2in}

\begin{flushleft}
{\Large
\textbf\newline{Automatic interpretation of cod otoliths using deep learning}
}
\newline


Endre Moen\textsuperscript{1*},
Rune Vabø\textsuperscript{1},
Come Denechaud\textsuperscript{1},
Ketil Malde\textsuperscript{1,2},
\\
\bigskip
\textbf{1} Institute of Marine Research, Bergen, Norway
\\
\textbf{2} Department of Informatics, University of Bergen, Norway
\\
\bigskip
* endre.moen@hi.no

\end{flushleft}
% Please keep the abstract below 300 words

\linenumbers

\section*{Abstract}

The age of individual cod (Gadus morhua) is determined by manually examining the layered structure of otoliths, a calcium carbonate structure of the inner ear. Image-based methods have been tried to age otoliths with varying results, but recent developments in automatic image analysis techniques are promising. The objective of this paper is to
investigate the accuracy in aging broken otolith images on state-of-the-art convolutional neural networks.


\section*{Introduction}

Information on fish age constitutes one of the most important biological variables, which is used
in the investigations of life history (e.g. growth, sexual maturation) and population dynamics
\Mycite{campana2001accuracy}

– The need for more Automated analysis. On the future of data analysis.
– On the importance of determining age distributions for fisheries and ecosystem management.
– How Cod otoliths are measured manually. The importance of otoliths for age determination of various species. Specifics about cod.
– Related work. Halibut otoliths. Salmon scales. The Greeks.
Knowledge of fish age structure is central to the study of fish and stock dynamics. It informs on population growth and mortality and, with size distribution, is one of the main criteria used for determining the health of exploited populations and monitoring the effects of selective fishing (Hidalgo et al., 2011; Brunel and Piet, 2013). Changes in the age distribution can track significant changes in population structure, such as a particularly strong year-class skewing the distribution (Reglero and Mosegaard, 2006), or the gradual truncation of older age classes as selective fishing mortality removes larger individuals (Siskey et al., 2016).
Hard structures such as scales and otoliths are used worldwide as one of the primary sources of fish age estimates, due to their ability as natural physiological and environmental recorders to form regular, temporally resolved growth increments at the daily and annual levels (Campana, 2001; Francis and Campana, 2011; Albuquerque et al., 2019). While age is inferred from the “simple” counting of annual increments, the interpretation of this zonation pattern is species or even population-specific (Høie et al., 2009) and is based on precise knowledge of the timing of zone formation and of the correct identification of true and false zones (Panfili et al., 2002). This process therefore requires specific expertise and is subject to uncertainties in both between-reader precision and “true” age accuracy (Francis and Campana, 2011). Because those estimates are central to stock assessment, ageing errors or wrong interpretation of otolith zonation can have dramatic effects on the evaluation of fish biology and consequently stock size and structure (Tyler et al., 1989; Beamish and McFarlane, 1995; Ragonese, 2018). 
Otolith reading is also time and resource consuming. Training of expert readers can take up to several years depending on the species, and otoliths often undergo a long processing phase before the final age estimates can be produced (Carbonara and Follesa, 2019). This is particularly true for demersal fish species, like Atlantic cod (Gadus morhua), that have large opaque otoliths that can’t be read whole and need to be prepared. These routines vary between populations and institutes and range from direct reading of broken otoliths under a magnifying glass, to embedding, thin sectioning and finally imaging of the sections under a microscope. There has been a variety of methods proposed to automatically interpret otoliths, which range from one-dimensional data analysis like intensity transects (Mahé, 2009) to the more recent effort toward developing machine learning (ML) frameworks (Moen et al., 2018; Politikos et al., 2021). Despite fast progress the results remain mixed and often yield lower precision and consistency than those obtained by trained human readers, which limits the application of automated methods in real conditions.
However, one aspect that is often under considered by such studies are the practical time and cost benefits that implementing a functional ML framework would provide. As noted by Fisher and Hunter (2018) in their review of digital techniques for otolith analysis, “costs for human and machine ageing systems are broadly similar since a large part of the cost is associated with preparing the otolith sections”. As such, the net benefit of automated ageing routines is directly dependent on the ability to scale performance using a comparatively smaller number of samples than human readers or, alternatively, to train them on “rougher” data that can be produced faster and at a more efficient cost.
MORE ON CNN ETC?
In this study, we develop a deep learning network for estimating Atlantic cod age using multi-exposure images of broken otoliths set in place using simple plasticine. More on methods. Our results are positive and show the potential for developing automated pipelines that require minimum processing and could be able to produce near at-sea age estimates.


\section*{Method and materials}

\subsection*{Data Collection}

We sampled a data set of  5150 cod otolith images, which has been collected 
on different cruises and read by otolith experts. The images are taken during
cruises in the period 2012-2018 conducted by Institute of Marine Research (IMR).
There are six images with three light exposures and one rotation.
The expert readers has varied during this time period as has the configuration
for photographing the otoliths. 

\begin{figure}[h!]
  \caption{Otolith from 2016, read age: 6. With light exposure: medium, low, high, 
  then rotated 180 degrees and three new images}
  \centering
  \includegraphics[scale=0.015]{otolith/IMG_0457_2016_70021.JPG}
  \includegraphics[scale=0.015]{otolith/IMG_0458_2016_70021.JPG}
  \includegraphics[scale=0.015]{otolith/IMG_0459_2016_70021.JPG} 

  \includegraphics[scale=0.015]{otolith/IMG_0460_2016_70021.JPG}
  \includegraphics[scale=0.015]{otolith/IMG_0461_2016_70021.JPG}
  \includegraphics[scale=0.015]{otolith/IMG_0462_2016_70021.JPG}
  
  \label{marker1}
\end{figure}

The images is of size 3744 x 5616 which are re-scaled for training to between 384x384 to 512x512. The image light exposure varies depending on light condition outside, and are stored in the property 'ExposureTime' of the JPG file. Typically the exposure order is middle, dark, or light then a rotation of 180 degrees, and then middle, light, dark again. But the order might change, and
the given order is recovered by reading the metadata property of the jpeg and sorting the exposure time.

The otoliths are prepared for imaging by breaking them.
The process also involves a camera setup, a folder structure referencing 
age, survey and station number, lighting setup, mounting of camera, and 
finally camera capture. More information about this process
can be found in \Mycite{codOtolithsMyers}


\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.60]{distribution/age_distribution.png}
  \caption{Age distribution of all 5150 images}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.60]{distribution/age_distribution_test.png}
  \caption{Age distribution of 515 images from the test set}
\end{figure}

\subsection*{Convolutional neural network architecture}

There are two families of models used, EfficientNet B4-B6 \citep{DBLP:journals/corr/abs-1905-11946}  and EfficientNet V2 medium, and Large \citep{DBLP:journals/corr/abs-1905-11946} 

Each CNN was trained using transfer learning by loading ImageNet \citep{deng2009imagenet} weights. The image size varied between 380x380 and 528x528 for EfficientNet Bx, and 384x384 for EfficientNetV2 size medium, large. While test-set size prediction has been done both on 384x384 and larger resolutions 480x480 and 512x512 as described in the paper. To investigate the image-taking protocol described in \citep{codOtolithsMyers} we have also training on 9-channel images. Three images are stacked to produce a 9-channel image. Using Timm\citep{rw2019timm} the imageNet weights were duplicated on the input layer to accommodate 9 channels. The 3 images used are of dark, medium and light exposure of the first orientation.

We apply state-of-the-art CNNs based on performance on the ImageNet benchmark. The imageNet benchmark has a classification while we treate ageing the cod-otoliths as a regression problem. So the last layer of the CNNs has been modified to output a linear output. In the EfficientNetV2 family we have done this by applying multilayer perceptron layers going from 1280 output of last hidden layer to dense 256-layer, then a leakyRelu \citep{DBLP:journals/corr/XuWCL15} layer, then dense 32-layer, then a leakyRelu layer, and finally linear output layer. While in EfficientNetV1 we only change the softmax layer to a linear out layer.

To each fold we apply Standard scalar, which Standardize the age by removing the mean and scaling to unit variance, on the training set. The standard scalar is then applied to validation and test-set. To get the test-set predictions we inverse transform the standard scalar.

\begin{figure}[h!]
  \caption{Otolith from 2013, read age: 6. With light exposure: medium, low, high, 
  and expectation per channel of the three exposures. }
  \centering
  \includegraphics[scale=1.0]{otolith/2013_70174_Nr06_age09_IMG_0031_32_33.png}
  
  \label{marker1}
\end{figure}

\subsection*{Implementation and training}

EfficientNetV1 B4, B5, and B6 was implemented with  tensorflow \citep{abadi2016tensorflow} and keras \citep{keras} software packages in python. Computation was done using CUDA 11.1 and CuDNN with Nvidia(Nvidia Corp., Santa Clara, California) A6000 accelerator card with 48 GB of GPU memory,
EfficientNetV2 medium, and large was implemented with the pytorch \citep{PyTorch}  and TIMM citep{rw2019timm} sorfware package. Computation was done on P100 cards with 12 GB of GPU memory. Pretrained weights for EfficientNetV1 was available through Keras, and pretrained weights for EfficeintNetV2 was available through Timm.

Augmentation was applied to the training-set. The images were augmented using rotation between 0 and 360 degrees, and reflection by the vertical axis. The pixel values has a range between 0 and 255 which was normalized to between 0 and 1.
No other augmentation techniques was used like  cropping, shifting or shearing as it can result in loss of age structure information.

The augmented data set can produce 360*2*5150 = 3.708.000 possible images.
Depending on the augmentation factor and the number of images in a training cycle, the model will likely never see the same image twice.

The primary metric used for training the models is mean squared error (MSE)
while the primary metric used for evaluating the models and comparing it to expert readers is accuracy. Accuracy is obtained by rounding the floating point number predictions to nearest integer and comparing the age classification against the true labels.
To reach human level accuracy a score of 85\% or higher is required \citep{ref_needed}.

To get the most out of a small data-set we applied 10-fold cross-validation on 90\% of the data-set, 4635 images. Each fold of the 10 folds consists of 90\% of the cross-validation set and 81\% of the whole data-set, 4172 images for training. Each fold had then 463 images for validation which is 10\% of the cross-validation set, and 9\% of the whole data-set. Each model is training on the 4172 images and the model with the best MSE on the 463 images in the validation set is chosen. The best model on the validation set was then used to predict the age on the test-set, and the metric for accuracy and MSE was recorded. The test-set is chosen at random, while the 10-fold split is chosen using stratified-kfold split which preserves the distribution of the whole cross-validation set in each validation set. So the 463 images while have similar age distribution to that of the 4635 images in the cross-validation set. Both the test-set and the whole data-set follows a normal distribution with larges age-class, 5 year old, but with deviation in frequency in other age-classes.

The CNN hyper-parameters configurations varies a little between the two families of networks, but are kept the same within the families. Some hyper-parameters
that has been tuned are batch size, learning rate, k-fold size, weight decay, step size, number of epochs, early stopping, and
patience. To keep track of all these parameters we wrote the configuration to a JSON file, config.json. The configuration file has been written for each model trained, and the exact configuration can be found in the results section of the github page of this project (https://github.com/emoen/Deep-learning-for-regression-of-cod-otoliths)

As previously mentioned, we trained 10 models using 10-fold cross-validation which resulted in 10 predictions on the test-set. We then produce an ensemble prediction of the 10 models. Typically the ensemble prediction is better than any single fold prediction. Ensembles are better because they improve 
performance. An ensemble can make better predictions and achieve better performance than any single contributing model, just as more
experts will produce higher accuracy in predicting a single otolith.
Robustness; An ensemble reduces the spread or dispersion of the predictions and model performance.
This result can be improved further by taking ensemble predictions of ensembles.
We look at all ensembles from tuple-ensembles, consisting of 2 models, which produces an ensemble of 20 models, and triplet-ensembles consisting of 3 models, to ensemble of all models which produces an ensemble consisting of 180 models. 

By choosing the best model we are over fitting to the test-set, but 
selecting a subset of the best of these ensembles should produce a candidate ensemble of ensemble which will produce the best prediction on a hold-out test-set.


\section*{Results}

We have conducted a series of experiments on the EfficientNet family of CNNs, 
with different hyper-parameters and on images with light exposures 
from the set light, medium, dark.
Training has been done on the 10-fold cross validation set which produced 10 models.

In table \ref{table1} and table \ref{table2} are the accuracy and MSE metrics
for the ensembled predictions from the 10 fold training. It can be observed that in the efficientNetV1 family,
larger networks has better MSE, while accuracy is more fluctuating.
A similar pattern can be observed for the efficientNetV2 networks.
Howerver it seems like efficientNetV1 is better than V2 in both metrics unlike
the results observed on ImageNet.

\begin{center}
\begin{table}[hbt!]
\caption{Accuracy by light exposure and CNN architectures}
\begin{tabular}{ |l|c|c|c|c|c|c| }

\hline
MSE:light/CNN & B4 & B5 & B6 & Medium & Large & Xtra Large \\ \hline
min        & 72.8 & 74.4 & 73.4 & 67.0* & - & - \\ 
medium     & - & - & 74.4 & 72.4 & 71.8 & - \\ 
max        & - & - & - & - & - & - \\ 
9 channels & - & - & - & - & 71.7 & - \\ 
\hline
\label{table1}
\end{tabular}
\end{table}
\end{center}

\begin{center}
\begin{table}[hbt!]
\caption{MSE by light exposure and CNN architectures}
\begin{tabular}{ |l|c|c|c|c|c|c| }

\hline
ACC:light/CNN & B4 & B5 & B6 & Medium & Large & Xtra Large \\ \hline
min        & .277 & .277 & .272 & .331* & - & - \\ 
medium     & - & - & .262 & .292 & .280 & - \\ 
max        & - & - & - & - & - & - \\ 
9 channels & - & - & - & - & .281 & - \\ 
\hline
\label{table2}
\end{tabular}
\end{table}
\end{center}


\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{1.0\textwidth}
  \includegraphics[scale=0.75]{results/fold_prediction_V2_m.png}
    \caption{Sample of 25 predictions on a fold of training on EfficientNetV2 size medium with minimum light exposure, left number is prediction,
    and right number is age read
    
    }
   \label{marker3}
  \end{minipage}
  \hfill
\end{figure}

We compare the 10 fold prediction accuracy of all the models in a box plot in
figure \ref{marker4}, and for MSE in \ref{marker5}. The red line 
is the ensemble accuracy or MSE. The ensemble metric is either better than 
all the folds or in the upper quantile.

\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
  \includegraphics[scale=0.2]{results/box_plot_models_acc.png}
    \caption{Accuracy score of 5 models and red line is ensemble prediction accuracy}
   \label{marker4}
  \end{minipage}
  \hfill
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
  \includegraphics[scale=0.2]{results/box_plot_models_acc.png}
    \caption{MSE score of 5 models and red line is ensemble prediction MSE}
   \label{marker5}
  \end{minipage}
  \hfill
\end{figure}

By comparing the models on MSE we can see that larger models are better, e.g B6 has 
higher mean than B5 and B4, and large is better than medium. 
We also see that the EfficientNetV2 networks has higher  mean than the
first generation EfficientNet. However, this is not true for the ensemble
predictions (red line) nor for the fold-mean or ensemble of the accuracy.
We can also see that the effect of adding 3 images, creating 9 channels, on the
model is that the variance is reduced, the fold mean metric increases,
but the ensemble metric is reduced.

The box plots are produced from the folds given in table \ref{table3} and \ref{table4}.
\begin{table}[!ht]
    \caption{MSE per CNN and per fold}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        CNN/fold & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & mse \\ \hline
        B4, min & .320 & .318 & .306 & .313 & .322 & .314 & .315 & .316 & .306 & .302 & .277 \\ \hline
        B4, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B4, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B5, min & .324 & .322 & .325 & .336 & .291 & .314 & .320 & .331 & .33 & .317 & .277 \\ \hline
        B5, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B5, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B6, min & .325 & .329 & .334 & .293 & .312 & .290 & .320 & .3. & .276 & .306 & .272 \\ \hline
        B6, middle & .323 & .301 & .312 & .268 & .294 & .266 & .309 & .311 & .278 & .289 & .262 \\ \hline
        B6, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        medium, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        med., mid. & .321 & .377 & .332 & .285 & .285 & .325 & .311 & .348 & .295 & .373 & .292 \\ \hline
        medium, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        medium, all & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, middle & .301 & .281 & .299 & .318 & .282 & .305 & .280 & .334 & .3 & .310 & .280 \\ \hline
        large, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, all & .292 & .289 & .289 & .326 & .307 & .327 & .283 & .30 & .335 & .295 & .281 \\ \hline
        xl, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, all & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
    \label{table3}
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Accuracy per CNN and per fold}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        CNN/fold & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & acc \\ \hline
        B4, min & 69.9 & 68.9 & 68.7 & 68.3 & 68.9 & 70.1 & 69.7 & 66.8 & 68.9 & 72.4 & 72.8 \\ \hline
        B4, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B4, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B5, min & 71.8 & 69.1 & 69.3 & 66.8 & 73.6 & 70.7 & 66.2 & 68.3 & 69.5 & 68.7 & 74.4 \\ \hline
        B5, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B5, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B6, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        B6, middle & 68.3 & 68.5 & 66.4 & 72.4 & 70.7 & 70.9 & 69.3 & 69.3 & 72 & 68.9 & 73.4 \\ \hline
        B6, max & 68.5 & 69.9 & 67.6 & 73.6 & 72.8 & 72 & 68 & 69.3 & 72 & 71.1 & 74.4 \\ \hline
        medium, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        med., mid. & 68.7 & 67.6 & 68.3 & 71.1 & 70.1 & 70.5 & 69.9 & 68.3 & 69.9 & 66 & 72.4 \\ \hline
        medium, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        medium, all & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, middle & 69.7 & 73.4 & 69.1 & 67 & 71.8 & 69.9 & 72.6 & 68.2 & 70.5 & 70.3 & 71.8 \\ \hline
        large, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        large, all & 70.9 & 70.7 & 70.5 & 70.7 & 71.5 & 69.3 & 70.7 & 71.8 & 69.7 & 70.9 & 71.7 \\ \hline
        xl, min & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, middle & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, max & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
        xl, all & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
    \label{table4}
    \end{tabular}
\end{table}

\subsection*{Prediction by age class and residuals}

The figures below shows the predictions per age group on the test-set. We
can see that the prediction follows a linear trend $y=x$ except for the 2-3 last years,
when the mean drops below $y=x$. This is even more obvious in the residual plots
where the prediction drops below $y=0$ for the last 2-3 age groups.


\begin{figure}
  \centering
  \subfloat[B4 minimum exposure]{\label{ref_label1}\includegraphics[width=0.5\textwidth]{result_pr_model/b4_min/boxplot_pr_age.png}}
  \subfloat[B4 minimum exposure residuals]{\label{ref_label2}\includegraphics[width=0.5\textwidth]{result_pr_model/b4_min/boxplot_residual.png}}

  \subfloat[B5 minimum exposure]{\label{ref_label3}\includegraphics[width=0.5\textwidth]{result_pr_model/b5_min/boxplot_pr_age.png}}
  \subfloat[B5 minimum exposure, residuals]{\label{ref_label4}\includegraphics[width=0.5\textwidth]{result_pr_model/b5_min/boxplot_residual.png}}
  
  \subfloat[B6 minimum exposure]{\label{ref_label3}\includegraphics[width=0.5\textwidth]{result_pr_model/b6_min/boxplot_pr_age.png}}
  \subfloat[B6 minimum exposure, residuals]{\label{ref_label4}\includegraphics[width=0.5\textwidth]{result_pr_model/b6_min/boxplot_residual.png}}  
  \caption{\label{ref_label_overall}Comparing the models, looking at age per age class, and the residuals per prediction}
  \label{marker6}
\end{figure}

Figure \ref{marker7} shows scatter plots of all predictions that results in a misclassification. That is predictions
that error greater than 0.5 in magnitude. Predictions that miss by more than 1.5 in magnitude are shown with red dots.

\begin{figure}
  \centering
  \subfloat[B4 minimum exposure]{\label{ref_label1}\includegraphics[width=0.5\textwidth]{result_pr_model/b4_min/misclassification.png}}

  \subfloat[B5 minimum exposure]{\label{ref_label3}\includegraphics[width=0.5\textwidth]{result_pr_model/b5_min/misclassification.png}}
  
  \subfloat[B6 minimum exposure]{\label{ref_label3}\includegraphics[width=0.5\textwidth]{result_pr_model/b6_min/misclassification.png}}
  \caption{\label{ref_label_overall}Comparing the models, looking at age per age class, and the reciduals per perdiction}
  \label{marker7}
\end{figure}


\subsection*{Ensemble of ensembles}

We search the space of ensemble of ensemble predictions
which are given by 
$\sum_{k=1}^{N}\binom{N}{k} $ where $N=22$ and $k \in 1..N$
and find three ensemble of ensembles which produce the best
results overall with accuracy of 75.9\%, 76.1\%, and 76.9\%
and MSE 0.247, 0.248, and 0.248 from ensemble of 
all networks, ensemble of B4, B5 and B6 with min exposure,
and ensemble of B4, B5, B6 and middle with min exposure.


\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
  \includegraphics[scale=0.2]{results/eoe_acc.png}
    \caption{Ensemble of ensemble: accuracy of the 3 best models}
   \label{marker5}
  \end{minipage}
  \hfill
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
  \includegraphics[scale=0.2]{results/eoe_mse.png}
    \caption{Ensemble of ensemble: mse of the 3 best models}
   \label{marker5}
  \end{minipage}
  \hfill
\end{figure}

\begin{center}
\begin{table}[hbt!]
\caption{Accuracy/MSE pr ensemble of ensemble.
Eoe1 is ensemble of ensemble of all models, Eoe2 is for B4, B5 and B6,
and Eoe3 is Eoe2 plus efficientNetV2 medium.}
\begin{tabular}{ |l|c|c|c| }
\hline
score/ensemble & eoe1 & eoe2 & eoe3  \\ \hline
Acc & 75.9 & 76.1 & 76.9 \\ 
MSE & .247 & .248 & .248  \\ 
\hline
\end{tabular}
\end{table}
\end{center}



\subsection*{Outliers}

Looking at figure \ref{} we can see that the model under predicts the age of older otoliths. This pattern is especially observable for individuals read as 15 years and older. The oldest predication is 18 years while the test set contains individuals as old as 22 years. To better understand the bias, figure \ref{marker5} shows the 4 largest outliers from the test set which come from two pairs

\begin{table}[!ht]
    \centering
    \caption{Outliers with more than 1.5 year error. Index of image in test-set per model}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
        V2-m,mid. & V2-m,mid. & V2-l,all & V2-l,mid. & B4,min & B5,min &  B6,min & B6,mid.  \\ \hline
        ~ & ~ & ~ & ~ & 13 & 13 & 13 & 13  \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & 48 &   \\ \hline
        71 & 71 & 71 & 71 & 71 & 71 & 71 & 71  \\ \hline
        92 & 92 & ~ & ~ & ~ & ~ & ~ &   \\ \hline
        ~ & ~ & ~ & ~ & 270 & 270 & ~ & 270  \\ \hline
        279 & 279 & 279 & 279 & 279 & 279 & 279 & 279  \\ \hline
        ~ & ~ & 312 & 312 & ~ & ~ & ~ &   \\ \hline
        ~ & ~ & ~ & 320 & 320 & ~ & ~ &   \\ \hline
        362 & 362 & 362 & 362 & 362 & 362 & 362 & 362  \\ \hline
        342 & 342 & 342 & 342 & 342 & 342 & 342 & 342  \\ \hline
        369 & 369 & 369 & 369 & 369 & ~ & 369 & 369  \\ \hline
        ~ & ~ & ~ & 393 & ~ & ~ & 393 & 393  \\ \hline
        423 & 423 & 423 & 423 & ~ & ~ & ~ &   \\ \hline
        ~ & ~ & ~ & ~ & ~ & 444 & ~ &   \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & 502 & 502  \\ \hline
        ~ & ~ & ~ & ~ & ~ & ~ & ~ &   \\ \hline
        7 & 7 & 7 & 9 & 8 & 7 & 9 & 9  \\ \hline
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Outliers with more than 1.5 year error. Prediction and true age, per model}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
        Idx & V2-m,mid. & V2-l,all & V2-l,mid. & B4,min & B5,min &  B6,min & B6,mid. & Age  \\ \hline
        13 & ~ & ~ & ~ & 9.79 & 9.64 & 9.74 & 9.58 & 8  \\ \hline
        48 & ~ & ~ & ~ & ~ & ~ & 7.6 & ~ & 6  \\ \hline
        71 & 4.96 & 4.98 & 4.94 & 5.14 & 4.79 & 5.06 & 5.12 & 7  \\ \hline
        92 & 10.95 & ~ & ~ & ~ & ~ & ~ & ~ & 13  \\ \hline
        270 & ~ & ~ & ~ & 11.66 & 11.71 & ~ & 11.53 & 10  \\ \hline
        279 & 9.93 & 9.79 & 9.75 & 9.89 & 9.69 & 9.67 & 9.7 & 8  \\ \hline
        312 & ~ & 9.42 & 9.38 & ~ & ~ & ~ & ~ & 11  \\ \hline
        320 & ~ & ~ & 5.44 & 5.47 & ~ & ~ & ~ & 7  \\ \hline
        362 & 5.11 & 5.14 & 5.23 & 5.11 & 5.29 & 5.24 & 5.15 & 7  \\ \hline
        342 & 10.35 & 10.6 & 10.61 & 11.05 & 10.75 & 10.69 & 10.84 & 13  \\ \hline
        369 & 8.17 & 8.13 & 8.23 & 8.24 & ~ & 7.85 & 8.29 & 10  \\ \hline
        393 & ~ & ~ & 10.53 & ~ & ~ & 10.75 & 10.83 & 9  \\ \hline
        423 & 5.39 & 5.69 & 5.43 & ~ & ~ & ~ & ~ & 8  \\ \hline
        444 & ~ & ~ & ~ & ~ & 10.95 & ~ & ~ & 9  \\ \hline
        502 & ~ & ~ & ~ & ~ & ~ & 9.4 & 9.43 & 11  \\ \hline
    \end{tabular}
\end{table}

\begin{figure}[h!]
  \caption{The most common images miss predicted with more than 1.5 years}
  \centering
  \includegraphics[scale=0.08]{outliers/IMG_0284_13.JPG}
  \includegraphics[scale=0.08]{outliers/IMG_0230_71.JPG}
  \includegraphics[scale=0.08]{outliers/IMG_0104_270.JPG} 

  \includegraphics[scale=0.08]{outliers/IMG_0044_342.JPG}
  \includegraphics[scale=0.08]{outliers/IMG_0086_360.JPG}
  \includegraphics[scale=0.08]{outliers/IMG_0122_369.JPG}
  
  \label{marker10}
\end{figure}

Figure \ref{marker7} are the most commonly misclassified images 
with greatest magnitude of error.


\section*{Discussion}

During initial training we trained a B4 network on ca 2000 images and obtained
an accuracy of ca 60\%, later another 3000 images was added and the same
network was trained on ca 5000 images which resulted in accuracy of ca 70\%.
It could be interesting to investigating if adding another 3-5000 images
would increase the accuracy to 80\%.


\section*{References}

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
